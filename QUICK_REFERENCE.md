# Quick Reference: Data Integration Pipeline

## ðŸš€ Quick Start

```bash
# 1. Set environment
export GCP_PROJECT_ID=your-project-id

# 2. Run orchestration
adk run agents/orchestration

# 3. Run complete workflow
> Run complete workflow from my_staging to my_target
```

## ðŸ“‹ 4-Stage Pipeline

```
STAGE 1: Staging Loader  â†’  Load CSV from GCS to BigQuery
STAGE 2: Schema Mapping  â†’  Generate AI-powered mappings
STAGE 3: Validation      â†’  Validate data quality
STAGE 4: ETL Agent       â†’  Generate & execute SQL
```

## ðŸŽ® Common Commands

### Complete Workflow
```
> Run complete workflow from [source] to [target]
```

### Step-by-Step
```
> Load [file] from [bucket] to [dataset]
> Generate schema mapping from [source] to [target]
> Validate the mapping
> Generate ETL SQL from the mapping
> Execute the ETL SQL in [target]
```

### Discovery
```
> Find schema files in [bucket]
> List all workflows
> List all mappings
> List all ETL scripts
```

### Status & Details
```
> Show workflow status
> Show me the mapping
> Show me the SQL
> Get validation results
```

## ðŸ”§ Environment Variables

```bash
# Required
export GCP_PROJECT_ID=your-project-id

# Optional
export GOOGLE_CLOUD_LOCATION=us-central1
```

## ðŸ“Š Workflow Tracking

Every workflow gets an ID:
```
workflow_20251216_103000
```

Use it to track progress:
```
> Show workflow status for workflow_20251216_103000
```

## ðŸ” Safety

âœ… SQL is **always** shown before execution  
âœ… User **must** approve before executing  
âœ… All operations are **tracked**  
âœ… State is **preserved** in session  

## ðŸ“¦ Project Structure

```
agents/
â”œâ”€â”€ orchestration/          # Main coordinator
â”œâ”€â”€ staging_loader_agent/   # Stage 1
â”œâ”€â”€ schema_mapping/         # Stage 2
â”œâ”€â”€ validation/             # Stage 3
â””â”€â”€ etl_agent/             # Stage 4
```

## ðŸ§ª Testing

```bash
# Test complete integration
python agents/orchestration/test_etl_integration.py

# Test individual agents
adk run agents/[agent_name]
```

## ðŸ“š Documentation

- [Complete Workflow Guide](agents/orchestration/COMPLETE_WORKFLOW_GUIDE.md)
- [ETL Agent README](agents/etl_agent/README.md)
- [Integration Summary](INTEGRATION_COMPLETE.md)

## ðŸŽ¯ Example Workflows

### World Bank Data
```
1. Load worldbank CSVs to staging
2. Map staging to target (star schema)
3. Validate data quality
4. Generate and execute ETL SQL
5. Ready for analytics!
```

### Commercial Lending
```
1. Load loan/borrower/collateral data
2. Map to dimensional model
3. Validate referential integrity
4. Execute ETL to data warehouse
5. Business intelligence ready!
```

## âš¡ Tips

- Use **FIX mode** for comprehensive mappings
- Use **REPORT mode** for validation
- Always **review SQL** before executing
- **Track workflows** with IDs
- **Test with small datasets** first

## ðŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| "GCP_PROJECT_ID not set" | `export GCP_PROJECT_ID=your-project-id` |
| "Mapping not found" | `> List all mappings` |
| "Table not found" | Ensure staging data is loaded |
| "Validation failed" | Review results, fix data, retry |
| "SQL execution failed" | Review SQL, check schemas, test single table |

## ðŸŽ‰ Summary

**4 Stages** â†’ **Complete Pipeline** â†’ **Production Ready**

```
Load â†’ Map â†’ Validate â†’ ETL = ðŸŽ¯ Data Integration!
```

---

**Quick Help**: `adk run agents/orchestration` then ask questions!

