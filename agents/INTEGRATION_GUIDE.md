# Schema Mapping + Data Validation Integration Guide

Complete guide for the integrated schema mapping and data validation workflow.

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│                  SCHEMA MAPPING AGENT                       │
│                  (Gemini 2.5 Flash)                         │
│                                                             │
│  Input:                                                     │
│  • Source Dataset: worldbank_staging_dataset                │
│  • Target Dataset: worldbank_target_dataset                 │
│  • Mode: REPORT or FIX                                      │
│                                                             │
│  Output:                                                    │
│  • Table mappings (source → target)                         │
│  • Column mappings with transformations                     │
│  • Validation rules (NOT_NULL, RANGE, UNIQUENESS, etc.)     │
│  • DEFAULT transformations for unmapped columns             │
│                                                             │
└──────────────────┬──────────────────────────────────────────┘
                   │
                   │ schema_mapping.json
                   │
                   ▼
┌─────────────────────────────────────────────────────────────┐
│                  DATA VALIDATION AGENT                      │
│                  (Gemini 2.5 Flash)                         │
│                                                             │
│  Input:                                                     │
│  • Schema mapping JSON                                      │
│  • Source dataset name                                      │
│  • Mode: REPORT or FIX                                      │
│                                                             │
│  Process:                                                   │
│  1. LLM generates SQL validation queries                    │
│  2. Execute queries on BigQuery staging tables              │
│  3. Log errors to staging_errors table                      │
│                                                             │
│  Output:                                                    │
│  • staging_errors table with all validation failures        │
│  • Run ID for tracking                                      │
│  • Summary report                                           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

## Complete Workflow

### Step 1: Generate Schema Mapping

```bash
cd agents/schema_mapping

# Generate mapping in FIX mode (with intelligent defaults)
python run_schema_mapper.py

# Or use the API
curl -X POST https://schema-mapping-api-313669899210.us-central1.run.app/generate-mapping \
  -H 'Content-Type: application/json' \
  -d '{
    "source_dataset": "worldbank_staging_dataset",
    "target_dataset": "worldbank_target_dataset",
    "mode": "FIX",
    "project_id": "ccibt-hack25ww7-750"
  }' > worldbank_mapping_fix.json
```

**Output:** `worldbank_mapping_fix.json` with:
- Table mappings
- Column mappings with transformations
- Validation rules
- PRIMARY KEY definitions
- UNIQUENESS constraints

### Step 2: Run Data Validation

```bash
cd agents/validation

# Validate using the schema mapping
python data_validator.py \
  ../schema_mapping/worldbank_mapping_fix.json \
  worldbank_staging_dataset \
  REPORT
```

**Output:**
- SQL validation queries generated by LLM
- Errors logged to `staging_errors` table
- Validation summary with run_id

### Step 3: Review Validation Errors

```sql
-- Query errors by run_id
SELECT
  source_table,
  target_table,
  error_type,
  failed_column,
  row_count,
  error_message,
  created_at
FROM `ccibt-hack25ww7-750.worldbank_staging_dataset.staging_errors`
WHERE run_id = 'YOUR_RUN_ID_HERE'
ORDER BY row_count DESC, error_type

-- Summary by error type
SELECT
  error_type,
  COUNT(*) as occurrences,
  SUM(row_count) as total_affected_rows
FROM `ccibt-hack25ww7-750.worldbank_staging_dataset.staging_errors`
WHERE run_id = 'YOUR_RUN_ID_HERE'
GROUP BY error_type
ORDER BY total_affected_rows DESC

-- Errors by table
SELECT
  source_table,
  target_table,
  COUNT(*) as error_count,
  SUM(row_count) as total_rows_with_errors
FROM `ccibt-hack25ww7-750.worldbank_staging_dataset.staging_errors`
WHERE run_id = 'YOUR_RUN_ID_HERE'
GROUP BY source_table, target_table
ORDER BY total_rows_with_errors DESC
```

## Example: WorldBank Dataset

### Input Data Structure

**Staging Tables:**
- `staging_countries` - Country metadata
- `staging_indicators_meta` - Indicator definitions
- `staging_gdp` - GDP data by country/year
- `staging_population` - Population data
- `staging_co2_emissions` - CO2 emissions data
- `staging_life_expectancy` - Life expectancy data
- `staging_primary_enrollment` - Education enrollment data
- `staging_poverty_headcount` - Poverty statistics

**Target Tables (Dimensional Model):**
- `dim_country` - Country dimension
- `dim_indicator` - Indicator dimension
- `dim_time` - Time dimension
- `fact_indicator_values` - Fact table for all indicators
- `agg_country_year` - Aggregated metrics by country/year

### Schema Mapping Output

```json
{
  "mappings": [
    {
      "source_table": "ccibt-hack25ww7-750.worldbank_staging_dataset.staging_gdp",
      "target_table": "ccibt-hack25ww7-750.worldbank_target_dataset.fact_indicator_values",
      "column_mappings": [
        {
          "source_column": "country_code",
          "target_column": "country_key",
          "transformation": null
        },
        {
          "source_column": "GENERATED",
          "target_column": "loaded_at",
          "source_type": "EXPRESSION",
          "transformation": "DEFAULT: CURRENT_TIMESTAMP()"
        }
      ],
      "validation_rules": [
        {
          "column": "country_key",
          "type": "NOT_NULL",
          "reason": "Primary key component"
        },
        {
          "column": "year",
          "type": "RANGE",
          "min": 1900,
          "max": 2100,
          "reason": "Valid year range"
        }
      ],
      "primary_key": ["country_key", "year", "indicator_code"],
      "uniqueness_constraints": ["country_key", "year", "indicator_code"]
    }
  ]
}
```

### Validation Queries Generated

**1. UNIQUENESS Check (Primary Key)**
```sql
SELECT COUNT(*) as error_count
FROM (
  SELECT country_code, year, indicator_code, COUNT(*) as cnt
  FROM `ccibt-hack25ww7-750.worldbank_staging_dataset.staging_gdp`
  GROUP BY country_code, year, indicator_code
  HAVING COUNT(*) > 1
)
```

**2. NOT_NULL Check**
```sql
SELECT COUNT(*) as error_count
FROM `ccibt-hack25ww7-750.worldbank_staging_dataset.staging_gdp`
WHERE country_code IS NULL
```

**3. RANGE Check**
```sql
SELECT COUNT(*) as error_count
FROM `ccibt-hack25ww7-750.worldbank_staging_dataset.staging_gdp`
WHERE year IS NOT NULL
  AND (year < 1900 OR year > 2100)
```

**4. TYPE_CONVERSION Check**
```sql
SELECT COUNT(*) as error_count
FROM `ccibt-hack25ww7-750.worldbank_staging_dataset.staging_gdp`
WHERE value IS NOT NULL
  AND SAFE_CAST(value AS NUMERIC) IS NULL
```

### Validation Results

```
============================================================
VALIDATION SUMMARY
============================================================
Run ID: 550e8400-e29b-41d4-a716-446655440000
Tables Validated: 8
Total Validations Run: 32
Total Errors Found: 127

Errors breakdown:
  • UNIQUENESS: 15 duplicate rows
  • NOT_NULL: 23 missing values
  • TYPE_CONVERSION: 45 invalid conversions
  • RANGE: 28 out-of-range values
  • NUMERIC: 16 non-numeric values

Errors logged to: staging_errors table
============================================================
```

## staging_errors Table Example

| run_id | source_table | target_table | error_type | failed_column | row_count | error_message | created_at |
|--------|-------------|--------------|------------|---------------|-----------|---------------|------------|
| 550e... | staging_gdp | fact_indicator_values | UNIQUENESS | country_code,year,indicator_code | 15 | Duplicate rows for primary key | 2025-12-16 14:30:00 |
| 550e... | staging_gdp | fact_indicator_values | NOT_NULL | country_code | 23 | NULL values in required column | 2025-12-16 14:30:05 |
| 550e... | staging_gdp | fact_indicator_values | TYPE_CONVERSION | value | 45 | Cannot convert to NUMERIC type | 2025-12-16 14:30:10 |
| 550e... | staging_gdp | fact_indicator_values | RANGE | year | 28 | Year outside valid range (1900-2100) | 2025-12-16 14:30:15 |

## API Integration

### Option 1: Cloud Run API + Python Validator

```bash
# Step 1: Generate mapping via API
curl -X POST https://schema-mapping-api-313669899210.us-central1.run.app/generate-mapping \
  -H 'Content-Type: application/json' \
  -d '{
    "source_dataset": "worldbank_staging_dataset",
    "target_dataset": "worldbank_target_dataset",
    "mode": "FIX",
    "project_id": "ccibt-hack25ww7-750"
  }' > mapping.json

# Step 2: Run validation
python agents/validation/data_validator.py \
  mapping.json \
  worldbank_staging_dataset \
  REPORT
```

### Option 2: Python-to-Python

```python
import json
from agents.schema_mapping.schema_mapper import generate_schema_mapping
from agents.validation.data_validator import validate_schema_mapping
import vertexai

# Initialize
project_id = "ccibt-hack25ww7-750"
vertexai.init(project=project_id, location="us-central1")

# Step 1: Generate schema mapping
mapping_result = generate_schema_mapping(
    source_dataset="worldbank_staging_dataset",
    target_dataset="worldbank_target_dataset",
    output_file="/tmp/mapping.json",
    mode="FIX"
)

# Step 2: Validate data
validation_result = validate_schema_mapping(
    schema_mapping_json="/tmp/mapping.json",
    source_dataset="worldbank_staging_dataset",
    mode="REPORT"
)

print(f"Validation Run ID: {validation_result['run_id']}")
print(f"Total Errors: {validation_result['total_errors']}")
```

## CI/CD Integration

### GitHub Actions Example

```yaml
name: Data Quality Validation

on:
  push:
    paths:
      - 'dataSets/**'
  schedule:
    - cron: '0 0 * * *'  # Daily

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r agents/schema_mapping/requirements.txt
          pip install -r agents/validation/requirements.txt

      - name: Authenticate to GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Generate Schema Mapping
        run: |
          cd agents/schema_mapping
          python run_schema_mapper.py

      - name: Run Data Validation
        run: |
          cd agents/validation
          python data_validator.py \
            ../schema_mapping/worldbank_mapping_fix.json \
            worldbank_staging_dataset \
            REPORT

      - name: Check for Critical Errors
        run: |
          # Query staging_errors and fail if critical errors found
          # (implement your threshold logic here)
```

## Monitoring & Alerting

### Query Validation Trends

```sql
-- Error trends over time
SELECT
  DATE(created_at) as validation_date,
  error_type,
  SUM(row_count) as total_errors
FROM `ccibt-hack25ww7-750.worldbank_staging_dataset.staging_errors`
WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
GROUP BY validation_date, error_type
ORDER BY validation_date DESC, total_errors DESC

-- Most problematic tables
SELECT
  source_table,
  COUNT(DISTINCT run_id) as validation_runs,
  COUNT(*) as error_occurrences,
  SUM(row_count) as total_rows_affected
FROM `ccibt-hack25ww7-750.worldbank_staging_dataset.staging_errors`
WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
GROUP BY source_table
ORDER BY total_rows_affected DESC
```

### Data Quality Metrics

```sql
-- Calculate data quality score
WITH error_stats AS (
  SELECT
    run_id,
    COUNT(*) as error_count,
    SUM(row_count) as total_error_rows
  FROM `ccibt-hack25ww7-750.worldbank_staging_dataset.staging_errors`
  GROUP BY run_id
),
table_stats AS (
  SELECT
    'staging_gdp' as table_name,
    COUNT(*) as total_rows
  FROM `ccibt-hack25ww7-750.worldbank_staging_dataset.staging_gdp`
)
SELECT
  e.run_id,
  t.total_rows,
  e.total_error_rows,
  ROUND((1 - e.total_error_rows / t.total_rows) * 100, 2) as quality_score_pct
FROM error_stats e
CROSS JOIN table_stats t
ORDER BY quality_score_pct DESC
```

## Best Practices

### 1. Schema Mapping

- ✅ Always use FIX mode to get intelligent default suggestions
- ✅ Review UNMAPPED columns in REPORT mode first
- ✅ Store mapping JSON in version control
- ✅ Re-generate mappings when schemas change

### 2. Data Validation

- ✅ Run validation before loading to target tables
- ✅ Use unique run_id for each validation run
- ✅ Monitor validation trends over time
- ✅ Set thresholds for acceptable error rates
- ✅ Archive old staging_errors data periodically

### 3. Error Handling

- ✅ Investigate UNIQUENESS errors first (data corruption)
- ✅ Fix NOT_NULL errors before type conversions
- ✅ Review RANGE errors for business rule violations
- ✅ Log all fixes for audit trail

## Troubleshooting

### Issue: No validation queries generated

**Cause**: LLM failed to parse schema mapping or generate queries

**Solution**:
- Check schema mapping JSON format
- Verify validation_rules are present in mapping
- Review LLM response in logs
- Try with simpler table mapping first

### Issue: SQL query execution fails

**Cause**: Invalid SQL syntax or permission issues

**Solution**:
- Check generated SQL in logs
- Verify BigQuery permissions
- Ensure source tables exist and are accessible
- Check for special characters in table/column names

### Issue: staging_errors table not created

**Cause**: Missing dataset or permission issues

**Solution**:
- Verify dataset exists in BigQuery
- Check BigQuery Admin permissions
- Manually create table if needed using schema in VALIDATOR_README.md

## Performance Optimization

- **Parallel Validation**: Validate multiple tables concurrently
- **Query Optimization**: Use LIMIT for large tables during testing
- **Batch Processing**: Process tables in batches of 5-10
- **Caching**: Cache schema mapping results for repeated runs
- **Partitioning**: Partition staging_errors by created_at for faster queries

## Cost Considerations

- **LLM Costs**: ~$0.10-0.50 per validation run (depends on table count)
- **BigQuery Costs**: Query execution charges apply
- **Storage**: staging_errors table grows over time - archive periodically
- **API Calls**: Schema Mapping API costs ~$0.05-0.20 per request

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0.0 | 2025-12-16 | Initial release with LLM-powered validation |

## Support

For issues or questions:
1. Check [Schema Mapping API README](schema_mapping/API_README.md)
2. Check [Validator README](validation/VALIDATOR_README.md)
3. Review sample responses in `schema_mapping/sample_api_response_*.json`
